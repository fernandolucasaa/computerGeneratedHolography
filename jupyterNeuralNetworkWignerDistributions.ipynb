{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning study on the results of the 1D Pseudo-Wigner Distribution using Neural Networks\n",
    "\n",
    "In this test, we are going to use the Wigner distribution of the datasets created by octave codes.\n",
    "\n",
    "Some references for the CNN application with keras (python):\n",
    "* https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wd_results.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ca4204caedc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Laod the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wd_results.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ferna\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wd_results.npy'"
     ]
    }
   ],
   "source": [
    "# Laod the dataset\n",
    "\n",
    "dataset = np.load('wd_results.npy')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_targets_array(nb_class, X_train):\n",
    "    \n",
    "    nb_holograms = X_train.shape[0]\n",
    "    nb_holograms_class = int(nb_holograms / nb_class)\n",
    "    \n",
    "    # Y array\n",
    "    Y_array = np.zeros((nb_holograms,))\n",
    "    counter = 1\n",
    "    target = 0 # 1\n",
    "    \n",
    "    for i in range(nb_holograms):\n",
    "        if counter == (nb_holograms_class + 1):\n",
    "            target = target + 1\n",
    "            counter = 1\n",
    "        Y_array[i,] = target\n",
    "        counter = counter + 1    \n",
    "    \n",
    "    return Y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 200, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data pre-procesing\n",
    "X_train = dataset[:,0,:,:]\n",
    "X_train = X_train.reshape(25,200,200,1)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4.\n",
      " 4.]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Compute array of targets\n",
    "nb_class = 5\n",
    "Y_array = compute_targets_array(nb_class, X_train)\n",
    "print(Y_array)\n",
    "\n",
    "# One-hot encode target column\n",
    "Y_train = to_categorical(Y_array)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential() # allows build a model layer by layer\n",
    "\n",
    "# Add model layers\n",
    "\n",
    "# Conv2D layer: \n",
    "# 64 nodes, 3x3 filter matrix, Rectified Linear Activation as activation function,\n",
    "# shape of each input (200, 200, 1,) with 1 signifying images are greyscale\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(200,200,1))) \n",
    "\n",
    "# 32 nodes\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Flatten layer: connection between the convolution and dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer: used for the output layer\n",
    "# 5 nodes for the output layer, one for each possible outcome (1-5)\n",
    "# 'softmax' as activation function, it makes the output sump up to 1 so the output\n",
    "# can be interpreted as probalities\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compiling the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three parameters:\n",
    "# optmizer: 'adam'\n",
    "# loss function: 'categorical_crossentropy', the most common choice for classification\n",
    "# metrics: 'accuracy', to see the accuracy score\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25 samples, validate on 25 samples\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 5s 204ms/step - loss: 13574.5508 - accuracy: 0.2400 - val_loss: 2178413.0000 - val_accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 5s 184ms/step - loss: 2178413.2500 - accuracy: 0.4000 - val_loss: 1573390.7500 - val_accuracy: 0.2800\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 5s 181ms/step - loss: 1573390.7500 - accuracy: 0.2800 - val_loss: 996428.9375 - val_accuracy: 0.3200\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 5s 182ms/step - loss: 996428.9375 - accuracy: 0.3200 - val_loss: 599865.6875 - val_accuracy: 0.4000\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 5s 185ms/step - loss: 599865.6875 - accuracy: 0.4000 - val_loss: 332944.6562 - val_accuracy: 0.4000\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 5s 186ms/step - loss: 332944.6562 - accuracy: 0.4000 - val_loss: 111936.1797 - val_accuracy: 0.4400\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 5s 218ms/step - loss: 111936.1719 - accuracy: 0.4400 - val_loss: 6546.9473 - val_accuracy: 0.8400\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 5s 201ms/step - loss: 6546.9473 - accuracy: 0.8400 - val_loss: 158427.0938 - val_accuracy: 0.6000\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 5s 192ms/step - loss: 158427.0938 - accuracy: 0.6000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 5s 197ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1427cedf4c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of epochs: number of tmes the model wil cycle trough the data\n",
    "model.fit(X_train, Y_train, validation_data=(X_train, Y_train), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evalutation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (expected 0)\n",
      "0 (expected 0)\n",
      "0 (expected 0)\n",
      "0 (expected 0)\n",
      "0 (expected 0)\n"
     ]
    }
   ],
   "source": [
    "# Make probability predictions with the model\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "# Round predictions \n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "\n",
    "# Make class predictions with the model\n",
    "predictions = model.predict_classes(X_train)\n",
    "\n",
    "# Summarize the first 5 cases\n",
    "for i in range(5):\n",
    "    print('%d (expected %d)' % (predictions[i], Y_array[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save weights and model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model structure and weights\n"
     ]
    }
   ],
   "source": [
    "# Serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"output/neural_networks/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# Serialize weights to HDF5\n",
    "model.save_weights(\"output/neural_networks/model.h5\")\n",
    "print(\"Saved model structure and weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# The model weights and architecture were saved separated, so it must re-compile\\n\\n# Load json and create model\\njson_file = open(\\'model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\nloaded_model = model_from_json(loaded_model_json)\\n\\n# Load weights into new model\\nloaded_model.load_weights(\"model.h5\")\\nprint(\"Loaded model from disk\")\\n \\n# Evaluate loaded model on test data\\nloaded_model.compile(loss=\\'categorical_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])\\nscore = loaded_model.evaluate(X_train, Y_train, verbose=0)\\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The model weights and architecture were saved separated, so it must re-compile\n",
    "\n",
    "# Load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 198, 198, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 196, 196, 32)      18464     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1229312)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 6146565   \n",
      "=================================================================\n",
      "Total params: 6,165,669\n",
      "Trainable params: 6,165,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error\n",
    "\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brouillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "visible = Input(shape=(64,64,1))\n",
    "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "flat = Flatten()(pool2)\n",
    "hidden1 = Dense(10, activation='relu')(flat)\n",
    "output = Dense(1, activation='sigmoid')(hidden1)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "# plot graph\n",
    "plot_model(model, to_file='convolutional_neural_network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
