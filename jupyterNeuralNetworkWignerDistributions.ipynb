{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning study on the results of the 1D Pseudo-Wigner Distribution using Neural Networks\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Check if the wigner distribution of an hologram is capable to give us enough information to be able to predict how many point sources generated the hologram (1 to 5 sources).\n",
    "\n",
    "**How?**\n",
    "\n",
    "Using a Convolutional Neural Networks (CNN) to solve this classification problem.\n",
    "\n",
    "**What?**\n",
    "\n",
    "Using the keras libray (python).\n",
    "\n",
    "**Some examples:**\n",
    "* https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 8, 200, 200)\n",
      "Total number of holograms: 125\n",
      "Number of holograms per class: 25\n",
      "Wall time: 232 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = 'output/wigner_distribution/'\n",
    "file_name = 'wd_results.npy'\n",
    "\n",
    "dataset = np.load(path + file_name)\n",
    "print(dataset.shape)\n",
    "\n",
    "print('Total number of holograms: ' + str(dataset.shape[0]))\n",
    "print('Number of holograms per class: ' + str(int(dataset.shape[0]/ 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_targets_array(nb_class, X_train):\n",
    "    \"\"\"\n",
    "    Compute an array with the targets of the dataset. Note that the number on the array correspond to the number of \n",
    "    sources minus one. E.g. Y_array = 1, the number of point sources is 2.\n",
    "    \"\"\"\n",
    "    # Number of the examples\n",
    "    nb_holograms = X_train.shape[0]\n",
    "    \n",
    "    # Number of examples per class\n",
    "    nb_holograms_class = int(nb_holograms / nb_class)\n",
    "    \n",
    "    # Y vector\n",
    "    Y_array = np.zeros((nb_holograms,))\n",
    "    counter = 1\n",
    "    target = 0\n",
    "    \n",
    "    for i in range(nb_holograms):\n",
    "        if counter == (nb_holograms_class + 1):\n",
    "            target = target + 1\n",
    "            counter = 1\n",
    "        Y_array[i,] = target\n",
    "        counter = counter + 1    \n",
    "    \n",
    "    return Y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 200, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Select one of the 8 frequencies ! BUG !!!!!!!!!!!!\n",
    "X_train = dataset[:,0,:,:]\n",
    "\n",
    "# The 1 signify that the images are greyscale\n",
    "X_train = X_train.reshape(X_train.shape[0], 200, 200,1)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125,)\n",
      "(125, 5)\n"
     ]
    }
   ],
   "source": [
    "# Compute array of targets\n",
    "nb_class = 5\n",
    "Y_array = compute_targets_array(nb_class, X_train)\n",
    "print(Y_array.shape)\n",
    "\n",
    "# One-hot encode target column\n",
    "Y_train = to_categorical(Y_array)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential() # allows build a model layer by layer\n",
    "\n",
    "# Add model layers\n",
    "\n",
    "# Conv2D layer: \n",
    "# 64 nodes, 3x3 filter matrix, Rectified Linear Activation as activation function,\n",
    "# shape of each input (200, 200, 1,) with 1 signifying images are greyscale\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(200,200,1))) \n",
    "\n",
    "# 32 nodes\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Flatten layer: connection between the convolution and dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense layer: used for the output layer\n",
    "# 5 nodes for the output layer, one for each possible outcome (1-5)\n",
    "# 'softmax' as activation function, it makes the output sump up to 1 so the output\n",
    "# can be interpreted as probalities\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three parameters:\n",
    "# optmizer: 'adam'\n",
    "# loss function: 'categorical_crossentropy', the most common choice for classification\n",
    "# metrics: 'accuracy', to see the accuracy score\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125 samples, validate on 125 samples\n",
      "Epoch 1/30\n",
      "125/125 [==============================] - 24s 189ms/step - loss: 1969604.1170 - accuracy: 0.2160 - val_loss: 1934728.2350 - val_accuracy: 0.2080\n",
      "Epoch 2/30\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 1007733.9370 - accuracy: 0.1920 - val_loss: 162199.3170 - val_accuracy: 0.2000\n",
      "Epoch 3/30\n",
      "125/125 [==============================] - 24s 193ms/step - loss: 62347.1701 - accuracy: 0.2640 - val_loss: 1971.5147 - val_accuracy: 0.6880\n",
      "Epoch 4/30\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 3407.0259 - accuracy: 0.6880 - val_loss: 299.7151 - val_accuracy: 0.8080\n",
      "Epoch 5/30\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 153.1435 - accuracy: 0.8320 - val_loss: 1.8502 - val_accuracy: 0.9280\n",
      "Epoch 6/30\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.8037 - accuracy: 0.9440 - val_loss: 0.4717 - val_accuracy: 0.9440\n",
      "Epoch 7/30\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.3462 - accuracy: 0.9520 - val_loss: 0.2682 - val_accuracy: 0.9440\n",
      "Epoch 8/30\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.2470 - accuracy: 0.9440 - val_loss: 0.2148 - val_accuracy: 0.9520\n",
      "Epoch 9/30\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.2090 - accuracy: 0.9440 - val_loss: 0.2364 - val_accuracy: 0.9440\n",
      "Epoch 10/30\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.2712 - accuracy: 0.9280 - val_loss: 0.2194 - val_accuracy: 0.9440\n",
      "Epoch 11/30\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2246 - accuracy: 0.9360 - val_loss: 0.2305 - val_accuracy: 0.9360\n",
      "Epoch 12/30\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.2343 - accuracy: 0.9360 - val_loss: 0.2324 - val_accuracy: 0.9360\n",
      "Epoch 13/30\n",
      "125/125 [==============================] - 24s 195ms/step - loss: 0.2335 - accuracy: 0.9360 - val_loss: 0.2276 - val_accuracy: 0.9360\n",
      "Epoch 14/30\n",
      "125/125 [==============================] - 24s 191ms/step - loss: 0.2265 - accuracy: 0.9360 - val_loss: 0.2226 - val_accuracy: 0.9360\n",
      "Epoch 15/30\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.2200 - accuracy: 0.9360 - val_loss: 0.2176 - val_accuracy: 0.9440\n",
      "Epoch 16/30\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.2156 - accuracy: 0.9440 - val_loss: 0.2132 - val_accuracy: 0.9440\n",
      "Epoch 17/30\n",
      "125/125 [==============================] - 24s 192ms/step - loss: 0.2125 - accuracy: 0.9440 - val_loss: 0.2087 - val_accuracy: 0.9440\n",
      "Epoch 18/30\n",
      "125/125 [==============================] - 23s 188ms/step - loss: 0.2070 - accuracy: 0.9440 - val_loss: 0.2047 - val_accuracy: 0.9440\n",
      "Epoch 19/30\n",
      "125/125 [==============================] - 23s 187ms/step - loss: 0.2035 - accuracy: 0.9440 - val_loss: 0.2007 - val_accuracy: 0.9440\n",
      "Epoch 20/30\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.1993 - accuracy: 0.9440 - val_loss: 0.1970 - val_accuracy: 0.9440\n",
      "Epoch 21/30\n",
      "125/125 [==============================] - 25s 197ms/step - loss: 0.1956 - accuracy: 0.9440 - val_loss: 0.1932 - val_accuracy: 0.9440\n",
      "Epoch 22/30\n",
      "125/125 [==============================] - 23s 183ms/step - loss: 0.1923 - accuracy: 0.9440 - val_loss: 0.1893 - val_accuracy: 0.9440\n",
      "Epoch 23/30\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.1880 - accuracy: 0.9440 - val_loss: 0.1857 - val_accuracy: 0.9440\n",
      "Epoch 24/30\n",
      "125/125 [==============================] - 23s 186ms/step - loss: 0.1839 - accuracy: 0.9440 - val_loss: 0.1823 - val_accuracy: 0.9440\n",
      "Epoch 25/30\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.1809 - accuracy: 0.9440 - val_loss: 0.1786 - val_accuracy: 0.9440\n",
      "Epoch 26/30\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.1774 - accuracy: 0.9520 - val_loss: 0.1750 - val_accuracy: 0.9520\n",
      "Epoch 27/30\n",
      "125/125 [==============================] - 24s 188ms/step - loss: 0.1742 - accuracy: 0.9520 - val_loss: 0.1713 - val_accuracy: 0.9520\n",
      "Epoch 28/30\n",
      "125/125 [==============================] - 23s 185ms/step - loss: 0.1702 - accuracy: 0.9520 - val_loss: 0.1679 - val_accuracy: 0.9520\n",
      "Epoch 29/30\n",
      "125/125 [==============================] - 24s 190ms/step - loss: 0.1670 - accuracy: 0.9520 - val_loss: 0.1645 - val_accuracy: 0.9520\n",
      "Epoch 30/30\n",
      "125/125 [==============================] - 23s 184ms/step - loss: 0.1639 - accuracy: 0.9520 - val_loss: 0.1610 - val_accuracy: 0.9600\n",
      "Wall time: 11min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14f1706a4c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Number of epochs: number of tmes the model wil cycle trough the data\n",
    "model.fit(X_train, Y_train, validation_data=(X_train, Y_train), epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Accuracy: %.2f%%' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0 (expected: 0)\n",
      "Predicted: 0 (expected: 0)\n",
      "Predicted: 0 (expected: 0)\n",
      "Predicted: 3 (expected: 0)\n",
      "Predicted: 0 (expected: 0)\n"
     ]
    }
   ],
   "source": [
    "# Make probability predictions with the model\n",
    "predictions = model.predict(X_train)\n",
    "\n",
    "# Round predictions \n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "\n",
    "# Make class predictions with the model\n",
    "predictions = model.predict_classes(X_train)\n",
    "\n",
    "# Summarize the first 5 cases\n",
    "for i in range(5):\n",
    "    print('Predicted: %d (expected: %d)' % (predictions[i], Y_array[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save weights and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model structure and weights\n",
      "Wall time: 241 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"output/neural_networks/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# Serialize weights to HDF5\n",
    "model.save_weights(\"output/neural_networks/model.h5\")\n",
    "print(\"Saved model structure and weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "# The model weights and architecture were saved separated, so it must re-compile\n",
    "\n",
    "# Load json and create model\n",
    "json_file = open('output/neural_networks/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into new model\n",
    "loaded_model.load_weights(\"output/neural_networks/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 198, 198, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 196, 196, 32)      18464     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1229312)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 6146565   \n",
      "=================================================================\n",
      "Total params: 6,165,669\n",
      "Trainable params: 6,165,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error, BUG, MUST FIX\n",
    "\n",
    "# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
