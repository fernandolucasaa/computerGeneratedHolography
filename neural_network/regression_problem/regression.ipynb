{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matlab_dictionary(file_path, file_name, key):\n",
    "\n",
    "    # Read mat file dictionary\n",
    "    dictionary = scipy.io.loadmat(file_path + file_name)\n",
    "\n",
    "    # Access item of a dictionary\n",
    "    array = dictionary[key]\n",
    "\n",
    "    return array\n",
    "\n",
    "def load_hologram_dataset(path, file_path, file_name, key):\n",
    "\n",
    "    # Load dictionary\n",
    "    dat = load_matlab_dictionary(file_path, file_name, key)\n",
    "\n",
    "    # Number of holograms\n",
    "    nb_holograms = dat.shape[2]\n",
    "\n",
    "    # Number of class\n",
    "    nb_class = 5\n",
    "\n",
    "    # Number of holograms per class\n",
    "    nb_holograms_class = int(nb_holograms/nb_class)\n",
    "\n",
    "    # Save npy file\n",
    "#     np.save('dat.npy', dat)\n",
    "\n",
    "    return dat, nb_holograms, nb_class, nb_holograms_class\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    # Current directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Directory path\n",
    "    path = str((Path(cwd).parent).parent)\n",
    "    file_path = path + '\\\\output\\\\dataset\\\\'\n",
    "\n",
    "    # File names\n",
    "    file_name = 'hDataset.mat'\n",
    "    key = 'hDataset'\n",
    "\n",
    "    print('----- Loading hologram dataset... -----')\n",
    "\n",
    "    # Load hologram (matfile dictionary)\n",
    "    dat, nb_holograms, nb_class, nb_holograms_class = load_hologram_dataset(path, file_path, file_name, key)\n",
    "\n",
    "    # Display results\n",
    "    print('Hologram dataset loaded (matlab file dictionary)')\n",
    "    print('Hologram dataset shape: ', dat.shape)\n",
    "    print('Total number of holograms: ' + str(nb_holograms))\n",
    "    print('Number of holograms per class: ' + str(nb_holograms_class))\n",
    "    print('Hologram dataset saved in .npy file!\\n')\n",
    "\n",
    "    return dat, nb_holograms, nb_class, nb_holograms_class\n",
    "\n",
    "def reshape_dataset(dat, nb_holograms):\n",
    "\n",
    "    # Dimensions\n",
    "    rows = dat.shape[0]\n",
    "    columns = dat.shape[1]\n",
    "\n",
    "    # Reshape the dataset so that the first dimension is the number of holograms\n",
    "    dat_r = np.ones([nb_holograms, rows, columns], dtype=complex)\n",
    "    for i in range(nb_holograms):\n",
    "        dat_r[i, :, :] = dat[:, :, i]\n",
    "\n",
    "    # Reshape the dataset to 1 dimension\n",
    "    data_1D = np.reshape(dat_r, (1500, int(rows*columns)))\n",
    "\n",
    "    return data_1D\n",
    "\n",
    "def normalize(arr):\n",
    "\n",
    "    max_value = np.max(np.max(arr, axis=0))\n",
    "    min_value = np.min(np.min(arr, axis=0))\n",
    "    arr = (arr - min_value) / (max_value - min_value)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def normalize_dataset(nb_holograms, data_1D):\n",
    "\n",
    "    # Normalize the examples\n",
    "    data_1D_norm = np.zeros([nb_holograms, data_1D.shape[1]], dtype=complex)\n",
    "\n",
    "    # Normalize each example\n",
    "    for i in range(nb_holograms):\n",
    "        data_1D_norm[i, :] = normalize(data_1D[i, :])\n",
    "\n",
    "    return data_1D_norm\n",
    "\n",
    "def compute_targets_array(nb_holograms, nb_class, nb_holograms_class):\n",
    "\n",
    "    # Compute array of targets\n",
    "    Y_array = np.ones([nb_holograms,])\n",
    "\n",
    "    pos = 0\n",
    "    for c in range(nb_class):\n",
    "        for h in range(nb_holograms_class):\n",
    "            Y_array[pos] = c\n",
    "            pos = pos + 1\n",
    "\n",
    "    # Save matrix\n",
    "#     np.save('Y_array.npy', Y_array)\n",
    "\n",
    "    return Y_array\n",
    "\n",
    "def pre_processing(dat, nb_holograms, nb_class, nb_holograms_class):\n",
    "\n",
    "    print('----- Data pre-procesing... -----')\n",
    "\n",
    "    # Reshape the dataset to 1 dimension\n",
    "    print('Reshaping dataset to 1 dimension...')\n",
    "    data_1D = reshape_dataset(dat, nb_holograms)\n",
    "    print('Dataset 1D shape: ', data_1D.shape)\n",
    "\n",
    "    # Normalize the data\n",
    "    print('Normalizing dataset...')\n",
    "    data_1D_norm = normalize_dataset(nb_holograms, data_1D)\n",
    "    print('Normalized dataset shape: ', data_1D_norm.shape)\n",
    "\n",
    "    # Compute array of targets\n",
    "    print('Computing Y_array...')\n",
    "    Y_array = compute_targets_array(nb_holograms, nb_class, nb_holograms_class)\n",
    "\n",
    "    # Verify\n",
    "    print('Y_array shape: ', Y_array.shape)\n",
    "    print(collections.Counter(Y_array))\n",
    "    print('Y_array saved in a .npy file!\\n')\n",
    "\n",
    "    return data_1D_norm, Y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Loading hologram dataset... -----\n",
      "Hologram dataset loaded (matlab file dictionary)\n",
      "Hologram dataset shape:  (200, 200, 1500)\n",
      "Total number of holograms: 1500\n",
      "Number of holograms per class: 300\n",
      "Hologram dataset saved in .npy file!\n",
      "\n",
      "----- Data pre-procesing... -----\n",
      "Reshaping dataset to 1 dimension...\n",
      "Dataset 1D shape:  (1500, 40000)\n",
      "Normalizing dataset...\n",
      "Normalized dataset shape:  (1500, 40000)\n",
      "Computing Y_array...\n",
      "Y_array shape:  (1500,)\n",
      "Counter({0.0: 300, 1.0: 300, 2.0: 300, 3.0: 300, 4.0: 300})\n",
      "Y_array saved in a .npy file!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dat, nb_holograms, nb_class, nb_holograms_class = load_dataset()\n",
    "\n",
    "# Prepare dateset (reshape, normalize, target's array)\n",
    "data_1D_norm, Y_array = pre_processing(dat, nb_holograms, nb_class, nb_holograms_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 40000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1D_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_1D_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points_dataset():\n",
    "    \n",
    "    # Current directory\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # Directory path\n",
    "    path = str((Path(cwd).parent).parent)\n",
    "    file_path = path + '\\\\output\\\\dataset\\\\'\n",
    "\n",
    "    # File names\n",
    "    file_name = 'pDataset.mat'\n",
    "    key = 'pDataset'\n",
    "\n",
    "    points = load_matlab_dictionary(file_path, file_name, key)\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_dataset = load_points_dataset()\n",
    "points_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_points_dataset(class_nb, nb_holograms_class):\n",
    "    \n",
    "    # Initial position in the points dataset\n",
    "    init = 0\n",
    "    for nb in range(1, class_nb):\n",
    "        init = nb * nb_holograms_class + init\n",
    "    \n",
    "    # Final position in the points dataset\n",
    "    fin = 0\n",
    "    for nb in range(1, class_nb + 1):\n",
    "        fin = nb * nb_holograms_class + fin\n",
    "    fin = fin - 1\n",
    "\n",
    "    return init, fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points_dataset_one_class(data, class_nb, nb_holograms_class):\n",
    "    \n",
    "    # Initial and final position in the points dataset\n",
    "    init, fin = positions_points_dataset(class_nb, nb_holograms_class)\n",
    "\n",
    "    points_one_class = data[init:(fin + 1), :]\n",
    "    \n",
    "    return points_one_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_nb = 1\n",
    "\n",
    "p_dataset  = load_points_dataset_one_class(points_dataset, class_nb, nb_holograms_class)\n",
    "p_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 40000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = dataset[0:300,:]\n",
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data = p_dataset[:,0]\n",
    "Y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.000422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.000538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.000960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.000786\n",
       "1    0.000523\n",
       "2   -0.000766\n",
       "3   -0.000760\n",
       "4   -0.000875\n",
       "..        ...\n",
       "295  0.000811\n",
       "296 -0.000422\n",
       "297  0.000612\n",
       "298 -0.000538\n",
       "299  0.000960\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(Y_data)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()  # build a model layer by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add model layers\n",
    "\n",
    "# First layer (hidden layer)\n",
    "nb_nodes1 = 1000\n",
    "input_dim1 = X_data.shape[1] # 40000 \n",
    "activation1 = 'relu' # Rectified linear unit (ReLU)\n",
    "\n",
    "model.add(Dense(nb_nodes1, input_dim=input_dim1, activation=activation1))\n",
    "\n",
    "# Second layer (hidden layer)\n",
    "nb_nodes2 = 400\n",
    "activation2 = 'relu' # Rectified linear unit (ReLU)\n",
    "\n",
    "model.add(Dense(nb_nodes2, activation=activation2))\n",
    "\n",
    "# Third layer (output layer)\n",
    "activation3 = 'softmax'\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              40001000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               400400    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 40,401,801\n",
      "Trainable params: 40,401,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_data\n",
    "Y_train = np.reshape(Y_data, (-1,1))\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_asarray.py:85: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 3s 11ms/step - loss: 5592.0955 - mse: 5592.0957\n",
      "Epoch 2/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 50.1365 - mse: 50.1365\n",
      "Epoch 3/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 13.6090 - mse: 13.6090\n",
      "Epoch 4/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 4.4229 - mse: 4.4229\n",
      "Epoch 5/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 1.6201 - mse: 1.6201\n",
      "Epoch 6/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.6099 - mse: 0.6099\n",
      "Epoch 7/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.3038 - mse: 0.3038\n",
      "Epoch 8/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.2691 - mse: 0.2691\n",
      "Epoch 9/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.1345 - mse: 0.1345\n",
      "Epoch 10/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0753 - mse: 0.0753\n",
      "Epoch 11/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0495 - mse: 0.0495\n",
      "Epoch 12/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0740 - mse: 0.0740\n",
      "Epoch 13/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0877 - mse: 0.0877\n",
      "Epoch 14/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0772 - mse: 0.0772\n",
      "Epoch 15/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0809 - mse: 0.0809\n",
      "Epoch 16/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0338 - mse: 0.0338\n",
      "Epoch 17/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0377 - mse: 0.0377\n",
      "Epoch 18/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0374 - mse: 0.0374\n",
      "Epoch 19/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0622 - mse: 0.0622\n",
      "Epoch 20/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0749 - mse: 0.0749\n",
      "Epoch 21/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0339 - mse: 0.0339\n",
      "Epoch 22/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0274 - mse: 0.0274\n",
      "Epoch 23/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0372 - mse: 0.0372\n",
      "Epoch 24/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0645 - mse: 0.0645\n",
      "Epoch 25/50\n",
      "300/300 [==============================] - 3s 12ms/step - loss: 0.0451 - mse: 0.0451\n",
      "Epoch 26/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0145 - mse: 0.0145\n",
      "Epoch 27/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0113 - mse: 0.0113\n",
      "Epoch 28/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0151 - mse: 0.0151\n",
      "Epoch 29/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0343 - mse: 0.0343\n",
      "Epoch 30/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0118 - mse: 0.0118\n",
      "Epoch 31/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0071 - mse: 0.0071\n",
      "Epoch 32/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0153 - mse: 0.0153\n",
      "Epoch 33/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0233 - mse: 0.0233\n",
      "Epoch 34/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0135 - mse: 0.0135\n",
      "Epoch 35/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0125 - mse: 0.0125\n",
      "Epoch 36/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0096 - mse: 0.0096\n",
      "Epoch 37/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0055 - mse: 0.0055\n",
      "Epoch 38/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0278 - mse: 0.0278\n",
      "Epoch 39/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0130 - mse: 0.0130\n",
      "Epoch 40/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0230 - mse: 0.0230\n",
      "Epoch 41/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0083 - mse: 0.0083\n",
      "Epoch 42/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0371 - mse: 0.0371\n",
      "Epoch 43/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.0700 - mse: 0.0700\n",
      "Epoch 44/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.6275 - mse: 0.6275\n",
      "Epoch 45/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 1.6580 - mse: 1.6580\n",
      "Epoch 46/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 2.6690 - mse: 2.6690\n",
      "Epoch 47/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 2.0937 - mse: 2.0937\n",
      "Epoch 48/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 2.0828 - mse: 2.0828\n",
      "Epoch 49/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 1.4448 - mse: 1.4448\n",
      "Epoch 50/50\n",
      "300/300 [==============================] - 3s 11ms/step - loss: 0.8253 - mse: 0.8253\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "nb_epochs = 50\n",
    "nb_batchs = 1000\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=nb_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.226264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.008546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.667060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.460076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.357089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.445419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.613450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-0.475568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.299968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.052923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0   -0.226264\n",
       "1   -0.008546\n",
       "2   -0.667060\n",
       "3   -0.460076\n",
       "4   -0.357089\n",
       "..        ...\n",
       "295 -0.445419\n",
       "296 -0.613450\n",
       "297 -0.475568\n",
       "298 -0.299968\n",
       "299 -0.052923\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_train)\n",
    "df = pd.DataFrame(predictions)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "┬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (300, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-d6dc87576ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnb_batchs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[1;31m# using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m--> 642\u001b[1;33m                     y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 raise ValueError(\n\u001b[0;32m    283\u001b[0m                     \u001b[1;34m'You are passing a target array of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                     \u001b[1;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m                     \u001b[1;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                     \u001b[1;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are passing a target array of shape (300, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "loss_fct = 'categorical_crossentropy' # Used for classification problem\n",
    "opt_fct = 'adam' # Popular version of Gradient Descent\n",
    "metrics_fct = ['accuracy'] # Classification problem\n",
    "\n",
    "model.compile(loss=loss_fct, optimizer=opt_fct, metrics=metrics_fct)\n",
    "\n",
    "# Train the model\n",
    "nb_epochs = 50\n",
    "nb_batchs = 1000\n",
    "\n",
    "history = model.fit(X_data, Y_data, epochs=nb_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>miles</th>\n",
       "      <th>debt</th>\n",
       "      <th>income</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4099</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2677</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>41576</td>\n",
       "      <td>6215</td>\n",
       "      <td>27754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>43172</td>\n",
       "      <td>7626</td>\n",
       "      <td>28256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>6979</td>\n",
       "      <td>8071</td>\n",
       "      <td>4438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8778</td>\n",
       "      <td>9829</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>4850</td>\n",
       "      <td>3470</td>\n",
       "      <td>4742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>9312</td>\n",
       "      <td>2720</td>\n",
       "      <td>12771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>51343</td>\n",
       "      <td>8713</td>\n",
       "      <td>28511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>3735</td>\n",
       "      <td>6406</td>\n",
       "      <td>6104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>963 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender  miles   debt  income  sales\n",
       "0     28       0     23      0    4099    620\n",
       "1     26       0     27      0    2677   1792\n",
       "2     30       1     58  41576    6215  27754\n",
       "3     26       1     25  43172    7626  28256\n",
       "4     20       1     17   6979    8071   4438\n",
       "..   ...     ...    ...    ...     ...    ...\n",
       "958   22       0     11   8778    9829   1593\n",
       "959   19       1     23   4850    3470   4742\n",
       "960   28       1     28   9312    2720  12771\n",
       "961   50       0     29  51343    8713  28511\n",
       "962   47       1     15   3735    6406   6104\n",
       "\n",
       "[963 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Variables\n",
    "# d = np.loadtxt(\"cars.csv\", delimiter=\",\")\n",
    "dataframe = pd.read_csv(\"cars.csv\", delimiter=\",\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(963,)\n",
      "<class 'numpy.ndarray'>\n",
      "(963, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "MinMaxScaler()\n",
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "dataset = dataframe.values\n",
    "x = dataset[:,0:5]\n",
    "y = dataset[:,5]\n",
    "print(y.shape)\n",
    "print(type(y))\n",
    "y = np.reshape(y, (-1,1))\n",
    "print(y.shape)\n",
    "print(type(y))\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(x))\n",
    "xscale=scaler_x.transform(x)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\flucasamar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 185\n",
      "Trainable params: 185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 577 samples, validate on 145 samples\n",
      "Epoch 1/150\n",
      "577/577 [==============================] - 0s 260us/sample - loss: 0.2518 - mean_squared_error: 0.2518 - mean_absolute_error: 0.3954 - val_loss: 0.2213 - val_mean_squared_error: 0.2213 - val_mean_absolute_error: 0.3762\n",
      "Epoch 2/150\n",
      "577/577 [==============================] - 0s 38us/sample - loss: 0.1607 - mean_squared_error: 0.1607 - mean_absolute_error: 0.2929 - val_loss: 0.1343 - val_mean_squared_error: 0.1343 - val_mean_absolute_error: 0.2788\n",
      "Epoch 3/150\n",
      "577/577 [==============================] - 0s 36us/sample - loss: 0.0934 - mean_squared_error: 0.0934 - mean_absolute_error: 0.2359 - val_loss: 0.0791 - val_mean_squared_error: 0.0791 - val_mean_absolute_error: 0.2357\n",
      "Epoch 4/150\n",
      "577/577 [==============================] - 0s 36us/sample - loss: 0.0645 - mean_squared_error: 0.0645 - mean_absolute_error: 0.2179 - val_loss: 0.0619 - val_mean_squared_error: 0.0619 - val_mean_absolute_error: 0.2212\n",
      "Epoch 5/150\n",
      "577/577 [==============================] - 0s 35us/sample - loss: 0.0581 - mean_squared_error: 0.0581 - mean_absolute_error: 0.2120 - val_loss: 0.0555 - val_mean_squared_error: 0.0555 - val_mean_absolute_error: 0.2097\n",
      "Epoch 6/150\n",
      "577/577 [==============================] - 0s 36us/sample - loss: 0.0519 - mean_squared_error: 0.0519 - mean_absolute_error: 0.2004 - val_loss: 0.0497 - val_mean_squared_error: 0.0497 - val_mean_absolute_error: 0.1979\n",
      "Epoch 7/150\n",
      "577/577 [==============================] - 0s 36us/sample - loss: 0.0441 - mean_squared_error: 0.0441 - mean_absolute_error: 0.1834 - val_loss: 0.0435 - val_mean_squared_error: 0.0435 - val_mean_absolute_error: 0.1819\n",
      "Epoch 8/150\n",
      "577/577 [==============================] - 0s 31us/sample - loss: 0.0366 - mean_squared_error: 0.0366 - mean_absolute_error: 0.1652 - val_loss: 0.0369 - val_mean_squared_error: 0.0369 - val_mean_absolute_error: 0.1642\n",
      "Epoch 9/150\n",
      "577/577 [==============================] - 0s 29us/sample - loss: 0.0299 - mean_squared_error: 0.0299 - mean_absolute_error: 0.1486 - val_loss: 0.0303 - val_mean_squared_error: 0.0303 - val_mean_absolute_error: 0.1464\n",
      "Epoch 10/150\n",
      "577/577 [==============================] - 0s 29us/sample - loss: 0.0244 - mean_squared_error: 0.0244 - mean_absolute_error: 0.1325 - val_loss: 0.0268 - val_mean_squared_error: 0.0268 - val_mean_absolute_error: 0.1334\n",
      "Epoch 11/150\n",
      "577/577 [==============================] - 0s 33us/sample - loss: 0.0204 - mean_squared_error: 0.0204 - mean_absolute_error: 0.1183 - val_loss: 0.0236 - val_mean_squared_error: 0.0236 - val_mean_absolute_error: 0.1228\n",
      "Epoch 12/150\n",
      "577/577 [==============================] - 0s 31us/sample - loss: 0.0178 - mean_squared_error: 0.0178 - mean_absolute_error: 0.1086 - val_loss: 0.0219 - val_mean_squared_error: 0.0219 - val_mean_absolute_error: 0.1152\n",
      "Epoch 13/150\n",
      "577/577 [==============================] - 0s 29us/sample - loss: 0.0160 - mean_squared_error: 0.0160 - mean_absolute_error: 0.1005 - val_loss: 0.0210 - val_mean_squared_error: 0.0210 - val_mean_absolute_error: 0.1114\n",
      "Epoch 14/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0149 - mean_squared_error: 0.0149 - mean_absolute_error: 0.0953 - val_loss: 0.0208 - val_mean_squared_error: 0.0208 - val_mean_absolute_error: 0.1088\n",
      "Epoch 15/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0142 - mean_squared_error: 0.0142 - mean_absolute_error: 0.0924 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1077\n",
      "Epoch 16/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0139 - mean_squared_error: 0.0139 - mean_absolute_error: 0.0913 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1068\n",
      "Epoch 17/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0136 - mean_squared_error: 0.0136 - mean_absolute_error: 0.0898 - val_loss: 0.0206 - val_mean_squared_error: 0.0206 - val_mean_absolute_error: 0.1058\n",
      "Epoch 18/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0135 - mean_squared_error: 0.0135 - mean_absolute_error: 0.0889 - val_loss: 0.0206 - val_mean_squared_error: 0.0206 - val_mean_absolute_error: 0.1055\n",
      "Epoch 19/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0134 - mean_squared_error: 0.0134 - mean_absolute_error: 0.0886 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1053\n",
      "Epoch 20/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0875 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1047\n",
      "Epoch 21/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0132 - mean_squared_error: 0.0132 - mean_absolute_error: 0.0878 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1044\n",
      "Epoch 22/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0130 - mean_squared_error: 0.0130 - mean_absolute_error: 0.0868 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1042\n",
      "Epoch 23/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0130 - mean_squared_error: 0.0130 - mean_absolute_error: 0.0862 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1039\n",
      "Epoch 24/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0130 - mean_squared_error: 0.0130 - mean_absolute_error: 0.0857 - val_loss: 0.0200 - val_mean_squared_error: 0.0200 - val_mean_absolute_error: 0.1039\n",
      "Epoch 25/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0129 - mean_squared_error: 0.0129 - mean_absolute_error: 0.0865 - val_loss: 0.0199 - val_mean_squared_error: 0.0199 - val_mean_absolute_error: 0.1035\n",
      "Epoch 26/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0127 - mean_squared_error: 0.0127 - mean_absolute_error: 0.0847 - val_loss: 0.0201 - val_mean_squared_error: 0.0201 - val_mean_absolute_error: 0.1026\n",
      "Epoch 27/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0128 - mean_squared_error: 0.0128 - mean_absolute_error: 0.0848 - val_loss: 0.0197 - val_mean_squared_error: 0.0197 - val_mean_absolute_error: 0.1024\n",
      "Epoch 28/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0127 - mean_squared_error: 0.0127 - mean_absolute_error: 0.0844 - val_loss: 0.0197 - val_mean_squared_error: 0.0197 - val_mean_absolute_error: 0.1021\n",
      "Epoch 29/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0126 - mean_squared_error: 0.0126 - mean_absolute_error: 0.0837 - val_loss: 0.0197 - val_mean_squared_error: 0.0197 - val_mean_absolute_error: 0.1016\n",
      "Epoch 30/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0127 - mean_squared_error: 0.0127 - mean_absolute_error: 0.0852 - val_loss: 0.0196 - val_mean_squared_error: 0.0196 - val_mean_absolute_error: 0.1014\n",
      "Epoch 31/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0124 - mean_squared_error: 0.0124 - mean_absolute_error: 0.0836 - val_loss: 0.0196 - val_mean_squared_error: 0.0196 - val_mean_absolute_error: 0.1010\n",
      "Epoch 32/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0124 - mean_squared_error: 0.0124 - mean_absolute_error: 0.0832 - val_loss: 0.0195 - val_mean_squared_error: 0.0195 - val_mean_absolute_error: 0.1012\n",
      "Epoch 33/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0125 - mean_squared_error: 0.0125 - mean_absolute_error: 0.0845 - val_loss: 0.0195 - val_mean_squared_error: 0.0195 - val_mean_absolute_error: 0.1005\n",
      "Epoch 34/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0123 - mean_squared_error: 0.0123 - mean_absolute_error: 0.0826 - val_loss: 0.0193 - val_mean_squared_error: 0.0193 - val_mean_absolute_error: 0.1003\n",
      "Epoch 35/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0124 - mean_squared_error: 0.0124 - mean_absolute_error: 0.0841 - val_loss: 0.0193 - val_mean_squared_error: 0.0193 - val_mean_absolute_error: 0.1000\n",
      "Epoch 36/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0125 - mean_squared_error: 0.0125 - mean_absolute_error: 0.0824 - val_loss: 0.0192 - val_mean_squared_error: 0.0192 - val_mean_absolute_error: 0.0997\n",
      "Epoch 37/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0123 - mean_squared_error: 0.0123 - mean_absolute_error: 0.0840 - val_loss: 0.0190 - val_mean_squared_error: 0.0190 - val_mean_absolute_error: 0.0998\n",
      "Epoch 38/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0122 - mean_squared_error: 0.0122 - mean_absolute_error: 0.0818 - val_loss: 0.0192 - val_mean_squared_error: 0.0192 - val_mean_absolute_error: 0.0991\n",
      "Epoch 39/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0121 - mean_squared_error: 0.0121 - mean_absolute_error: 0.0822 - val_loss: 0.0190 - val_mean_squared_error: 0.0190 - val_mean_absolute_error: 0.0994\n",
      "Epoch 40/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0120 - mean_squared_error: 0.0120 - mean_absolute_error: 0.0818 - val_loss: 0.0192 - val_mean_squared_error: 0.0192 - val_mean_absolute_error: 0.0988\n",
      "Epoch 41/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0120 - mean_squared_error: 0.0120 - mean_absolute_error: 0.0816 - val_loss: 0.0189 - val_mean_squared_error: 0.0189 - val_mean_absolute_error: 0.0984\n",
      "Epoch 42/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0120 - mean_squared_error: 0.0120 - mean_absolute_error: 0.0821 - val_loss: 0.0188 - val_mean_squared_error: 0.0188 - val_mean_absolute_error: 0.0985\n",
      "Epoch 43/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0119 - mean_squared_error: 0.0119 - mean_absolute_error: 0.0813 - val_loss: 0.0189 - val_mean_squared_error: 0.0189 - val_mean_absolute_error: 0.0978\n",
      "Epoch 44/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0119 - mean_squared_error: 0.0119 - mean_absolute_error: 0.0816 - val_loss: 0.0188 - val_mean_squared_error: 0.0188 - val_mean_absolute_error: 0.0982\n",
      "Epoch 45/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0119 - mean_squared_error: 0.0119 - mean_absolute_error: 0.0811 - val_loss: 0.0188 - val_mean_squared_error: 0.0188 - val_mean_absolute_error: 0.0975\n",
      "Epoch 46/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0118 - mean_squared_error: 0.0118 - mean_absolute_error: 0.0814 - val_loss: 0.0187 - val_mean_squared_error: 0.0187 - val_mean_absolute_error: 0.0975\n",
      "Epoch 47/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0117 - mean_squared_error: 0.0117 - mean_absolute_error: 0.0806 - val_loss: 0.0187 - val_mean_squared_error: 0.0187 - val_mean_absolute_error: 0.0972\n",
      "Epoch 48/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0117 - mean_squared_error: 0.0117 - mean_absolute_error: 0.0806 - val_loss: 0.0186 - val_mean_squared_error: 0.0186 - val_mean_absolute_error: 0.0975\n",
      "Epoch 49/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0117 - mean_squared_error: 0.0117 - mean_absolute_error: 0.0806 - val_loss: 0.0188 - val_mean_squared_error: 0.0188 - val_mean_absolute_error: 0.0968\n",
      "Epoch 50/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0117 - mean_squared_error: 0.0117 - mean_absolute_error: 0.0809 - val_loss: 0.0186 - val_mean_squared_error: 0.0186 - val_mean_absolute_error: 0.0970\n",
      "Epoch 51/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - mean_absolute_error: 0.0803 - val_loss: 0.0186 - val_mean_squared_error: 0.0186 - val_mean_absolute_error: 0.0967\n",
      "Epoch 52/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - mean_absolute_error: 0.0805 - val_loss: 0.0186 - val_mean_squared_error: 0.0186 - val_mean_absolute_error: 0.0969\n",
      "Epoch 53/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0117 - mean_squared_error: 0.0117 - mean_absolute_error: 0.0802 - val_loss: 0.0184 - val_mean_squared_error: 0.0184 - val_mean_absolute_error: 0.0972\n",
      "Epoch 54/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - mean_absolute_error: 0.0808 - val_loss: 0.0185 - val_mean_squared_error: 0.0185 - val_mean_absolute_error: 0.0962\n",
      "Epoch 55/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0115 - mean_squared_error: 0.0115 - mean_absolute_error: 0.0798 - val_loss: 0.0184 - val_mean_squared_error: 0.0184 - val_mean_absolute_error: 0.0964\n",
      "Epoch 56/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0115 - mean_squared_error: 0.0115 - mean_absolute_error: 0.0801 - val_loss: 0.0183 - val_mean_squared_error: 0.0183 - val_mean_absolute_error: 0.0967\n",
      "Epoch 57/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0117 - mean_squared_error: 0.0117 - mean_absolute_error: 0.0803 - val_loss: 0.0184 - val_mean_squared_error: 0.0184 - val_mean_absolute_error: 0.0969\n",
      "Epoch 58/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0117 - mean_squared_error: 0.0117 - mean_absolute_error: 0.0814 - val_loss: 0.0185 - val_mean_squared_error: 0.0185 - val_mean_absolute_error: 0.0953\n",
      "Epoch 59/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - mean_absolute_error: 0.0797 - val_loss: 0.0182 - val_mean_squared_error: 0.0182 - val_mean_absolute_error: 0.0964\n",
      "Epoch 60/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0118 - mean_squared_error: 0.0118 - mean_absolute_error: 0.0820 - val_loss: 0.0186 - val_mean_squared_error: 0.0186 - val_mean_absolute_error: 0.0951\n",
      "Epoch 61/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - mean_absolute_error: 0.0800 - val_loss: 0.0182 - val_mean_squared_error: 0.0182 - val_mean_absolute_error: 0.0972\n",
      "Epoch 62/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0115 - mean_squared_error: 0.0115 - mean_absolute_error: 0.0809 - val_loss: 0.0184 - val_mean_squared_error: 0.0184 - val_mean_absolute_error: 0.0953\n",
      "Epoch 63/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0114 - mean_squared_error: 0.0114 - mean_absolute_error: 0.0793 - val_loss: 0.0182 - val_mean_squared_error: 0.0182 - val_mean_absolute_error: 0.0958\n",
      "Epoch 64/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0113 - mean_squared_error: 0.0113 - mean_absolute_error: 0.0797 - val_loss: 0.0181 - val_mean_squared_error: 0.0181 - val_mean_absolute_error: 0.0953\n",
      "Epoch 65/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0113 - mean_squared_error: 0.0113 - mean_absolute_error: 0.0791 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0954\n",
      "Epoch 66/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0113 - mean_squared_error: 0.0113 - mean_absolute_error: 0.0792 - val_loss: 0.0181 - val_mean_squared_error: 0.0181 - val_mean_absolute_error: 0.0950\n",
      "Epoch 67/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0114 - mean_squared_error: 0.0114 - mean_absolute_error: 0.0798 - val_loss: 0.0182 - val_mean_squared_error: 0.0182 - val_mean_absolute_error: 0.0950\n",
      "Epoch 68/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0113 - mean_squared_error: 0.0113 - mean_absolute_error: 0.0793 - val_loss: 0.0183 - val_mean_squared_error: 0.0183 - val_mean_absolute_error: 0.0948\n",
      "Epoch 69/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0113 - mean_squared_error: 0.0113 - mean_absolute_error: 0.0788 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0954\n",
      "Epoch 70/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0793 - val_loss: 0.0182 - val_mean_squared_error: 0.0182 - val_mean_absolute_error: 0.0944\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0789 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0953\n",
      "Epoch 72/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0792 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0945\n",
      "Epoch 73/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0788 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0957\n",
      "Epoch 74/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0792 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0943\n",
      "Epoch 75/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0788 - val_loss: 0.0179 - val_mean_squared_error: 0.0179 - val_mean_absolute_error: 0.0940\n",
      "Epoch 76/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0791 - val_loss: 0.0179 - val_mean_squared_error: 0.0179 - val_mean_absolute_error: 0.0940\n",
      "Epoch 77/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0111 - mean_squared_error: 0.0111 - mean_absolute_error: 0.0783 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0940\n",
      "Epoch 78/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0793 - val_loss: 0.0179 - val_mean_squared_error: 0.0179 - val_mean_absolute_error: 0.0938\n",
      "Epoch 79/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0114 - mean_squared_error: 0.0114 - mean_absolute_error: 0.0788 - val_loss: 0.0179 - val_mean_squared_error: 0.0179 - val_mean_absolute_error: 0.0960\n",
      "Epoch 80/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0799 - val_loss: 0.0182 - val_mean_squared_error: 0.0182 - val_mean_absolute_error: 0.0934\n",
      "Epoch 81/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0113 - mean_squared_error: 0.0113 - mean_absolute_error: 0.0794 - val_loss: 0.0179 - val_mean_squared_error: 0.0179 - val_mean_absolute_error: 0.0962\n",
      "Epoch 82/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0791 - val_loss: 0.0178 - val_mean_squared_error: 0.0178 - val_mean_absolute_error: 0.0937\n",
      "Epoch 83/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - mean_absolute_error: 0.0786 - val_loss: 0.0177 - val_mean_squared_error: 0.0177 - val_mean_absolute_error: 0.0937\n",
      "Epoch 84/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - mean_absolute_error: 0.0778 - val_loss: 0.0177 - val_mean_squared_error: 0.0177 - val_mean_absolute_error: 0.0933\n",
      "Epoch 85/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0109 - mean_squared_error: 0.0109 - mean_absolute_error: 0.0778 - val_loss: 0.0177 - val_mean_squared_error: 0.0177 - val_mean_absolute_error: 0.0934\n",
      "Epoch 86/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - mean_absolute_error: 0.0778 - val_loss: 0.0177 - val_mean_squared_error: 0.0177 - val_mean_absolute_error: 0.0937\n",
      "Epoch 87/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - mean_absolute_error: 0.0784 - val_loss: 0.0177 - val_mean_squared_error: 0.0177 - val_mean_absolute_error: 0.0932\n",
      "Epoch 88/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0109 - mean_squared_error: 0.0109 - mean_absolute_error: 0.0777 - val_loss: 0.0176 - val_mean_squared_error: 0.0176 - val_mean_absolute_error: 0.0937\n",
      "Epoch 89/150\n",
      "577/577 [==============================] - 0s 22us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - mean_absolute_error: 0.0786 - val_loss: 0.0180 - val_mean_squared_error: 0.0180 - val_mean_absolute_error: 0.0921\n",
      "Epoch 90/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0112 - mean_squared_error: 0.0112 - mean_absolute_error: 0.0781 - val_loss: 0.0177 - val_mean_squared_error: 0.0177 - val_mean_absolute_error: 0.0935\n",
      "Epoch 91/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0111 - mean_squared_error: 0.0111 - mean_absolute_error: 0.0778 - val_loss: 0.0176 - val_mean_squared_error: 0.0176 - val_mean_absolute_error: 0.0937\n",
      "Epoch 92/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0109 - mean_squared_error: 0.0109 - mean_absolute_error: 0.0778 - val_loss: 0.0177 - val_mean_squared_error: 0.0177 - val_mean_absolute_error: 0.0926\n",
      "Epoch 93/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0109 - mean_squared_error: 0.0109 - mean_absolute_error: 0.0774 - val_loss: 0.0175 - val_mean_squared_error: 0.0175 - val_mean_absolute_error: 0.0931\n",
      "Epoch 94/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - mean_absolute_error: 0.0788 - val_loss: 0.0178 - val_mean_squared_error: 0.0178 - val_mean_absolute_error: 0.0919\n",
      "Epoch 95/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0111 - mean_squared_error: 0.0111 - mean_absolute_error: 0.0776 - val_loss: 0.0175 - val_mean_squared_error: 0.0175 - val_mean_absolute_error: 0.0947\n",
      "Epoch 96/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0109 - mean_squared_error: 0.0109 - mean_absolute_error: 0.0784 - val_loss: 0.0179 - val_mean_squared_error: 0.0179 - val_mean_absolute_error: 0.0917\n",
      "Epoch 97/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0110 - mean_squared_error: 0.0110 - mean_absolute_error: 0.0773 - val_loss: 0.0176 - val_mean_squared_error: 0.0176 - val_mean_absolute_error: 0.0921\n",
      "Epoch 98/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0108 - mean_squared_error: 0.0108 - mean_absolute_error: 0.0770 - val_loss: 0.0175 - val_mean_squared_error: 0.0175 - val_mean_absolute_error: 0.0921\n",
      "Epoch 99/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0109 - mean_squared_error: 0.0109 - mean_absolute_error: 0.0778 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.0930\n",
      "Epoch 100/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0771 - val_loss: 0.0174 - val_mean_squared_error: 0.0174 - val_mean_absolute_error: 0.0923\n",
      "Epoch 101/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0108 - mean_squared_error: 0.0108 - mean_absolute_error: 0.0772 - val_loss: 0.0175 - val_mean_squared_error: 0.0175 - val_mean_absolute_error: 0.0925\n",
      "Epoch 102/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0109 - mean_squared_error: 0.0109 - mean_absolute_error: 0.0782 - val_loss: 0.0176 - val_mean_squared_error: 0.0176 - val_mean_absolute_error: 0.0911\n",
      "Epoch 103/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0768 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.0931\n",
      "Epoch 104/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0770 - val_loss: 0.0175 - val_mean_squared_error: 0.0175 - val_mean_absolute_error: 0.0912\n",
      "Epoch 105/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0771 - val_loss: 0.0175 - val_mean_squared_error: 0.0175 - val_mean_absolute_error: 0.0919\n",
      "Epoch 106/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0766 - val_loss: 0.0174 - val_mean_squared_error: 0.0174 - val_mean_absolute_error: 0.0916\n",
      "Epoch 107/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0766 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.0913\n",
      "Epoch 108/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0765 - val_loss: 0.0172 - val_mean_squared_error: 0.0172 - val_mean_absolute_error: 0.0918\n",
      "Epoch 109/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - mean_absolute_error: 0.0771 - val_loss: 0.0176 - val_mean_squared_error: 0.0176 - val_mean_absolute_error: 0.0907\n",
      "Epoch 110/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0108 - mean_squared_error: 0.0108 - mean_absolute_error: 0.0773 - val_loss: 0.0172 - val_mean_squared_error: 0.0172 - val_mean_absolute_error: 0.0922\n",
      "Epoch 111/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0764 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.0914\n",
      "Epoch 112/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0764 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.0906\n",
      "Epoch 113/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0761 - val_loss: 0.0172 - val_mean_squared_error: 0.0172 - val_mean_absolute_error: 0.0917\n",
      "Epoch 114/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0767 - val_loss: 0.0172 - val_mean_squared_error: 0.0172 - val_mean_absolute_error: 0.0909\n",
      "Epoch 115/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0763 - val_loss: 0.0171 - val_mean_squared_error: 0.0171 - val_mean_absolute_error: 0.0915\n",
      "Epoch 116/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0762 - val_loss: 0.0171 - val_mean_squared_error: 0.0171 - val_mean_absolute_error: 0.0913\n",
      "Epoch 117/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0763 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0909\n",
      "Epoch 118/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0769 - val_loss: 0.0172 - val_mean_squared_error: 0.0172 - val_mean_absolute_error: 0.0903\n",
      "Epoch 119/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0756 - val_loss: 0.0171 - val_mean_squared_error: 0.0171 - val_mean_absolute_error: 0.0910\n",
      "Epoch 120/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0761 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0906\n",
      "Epoch 121/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0759 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0906\n",
      "Epoch 122/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0756 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0901\n",
      "Epoch 123/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0757 - val_loss: 0.0171 - val_mean_squared_error: 0.0171 - val_mean_absolute_error: 0.0906\n",
      "Epoch 124/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0757 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0906\n",
      "Epoch 125/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0761 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0914\n",
      "Epoch 126/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0759 - val_loss: 0.0172 - val_mean_squared_error: 0.0172 - val_mean_absolute_error: 0.0893\n",
      "Epoch 127/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0770 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.0892\n",
      "Epoch 128/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0756 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0935\n",
      "Epoch 129/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0108 - mean_squared_error: 0.0108 - mean_absolute_error: 0.0773 - val_loss: 0.0171 - val_mean_squared_error: 0.0171 - val_mean_absolute_error: 0.0892\n",
      "Epoch 130/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - mean_absolute_error: 0.0766 - val_loss: 0.0169 - val_mean_squared_error: 0.0169 - val_mean_absolute_error: 0.0906\n",
      "Epoch 131/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0752 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0898\n",
      "Epoch 132/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0752 - val_loss: 0.0169 - val_mean_squared_error: 0.0169 - val_mean_absolute_error: 0.0895\n",
      "Epoch 133/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0752 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0893\n",
      "Epoch 134/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0752 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0905\n",
      "Epoch 135/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0758 - val_loss: 0.0169 - val_mean_squared_error: 0.0169 - val_mean_absolute_error: 0.0887\n",
      "Epoch 136/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0760 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0890\n",
      "Epoch 137/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0753 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0893\n",
      "Epoch 138/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0748 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0907\n",
      "Epoch 139/150\n",
      "577/577 [==============================] - 0s 26us/sample - loss: 0.0102 - mean_squared_error: 0.0102 - mean_absolute_error: 0.0751 - val_loss: 0.0172 - val_mean_squared_error: 0.0172 - val_mean_absolute_error: 0.0887\n",
      "Epoch 140/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0749 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0922\n",
      "Epoch 141/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0756 - val_loss: 0.0170 - val_mean_squared_error: 0.0170 - val_mean_absolute_error: 0.0888\n",
      "Epoch 142/150\n",
      "577/577 [==============================] - 0s 28us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0751 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0918\n",
      "Epoch 143/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0756 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0891\n",
      "Epoch 144/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0102 - mean_squared_error: 0.0102 - mean_absolute_error: 0.0749 - val_loss: 0.0167 - val_mean_squared_error: 0.0167 - val_mean_absolute_error: 0.0899\n",
      "Epoch 145/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0103 - mean_squared_error: 0.0103 - mean_absolute_error: 0.0754 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0892\n",
      "Epoch 146/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0761 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.0890\n",
      "Epoch 147/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0105 - mean_squared_error: 0.0105 - mean_absolute_error: 0.0756 - val_loss: 0.0167 - val_mean_squared_error: 0.0167 - val_mean_absolute_error: 0.0894\n",
      "Epoch 148/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0748 - val_loss: 0.0168 - val_mean_squared_error: 0.0168 - val_mean_absolute_error: 0.0910\n",
      "Epoch 149/150\n",
      "577/577 [==============================] - 0s 24us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0763 - val_loss: 0.0171 - val_mean_squared_error: 0.0171 - val_mean_absolute_error: 0.0883\n",
      "Epoch 150/150\n",
      "577/577 [==============================] - 0s 23us/sample - loss: 0.0104 - mean_squared_error: 0.0104 - mean_absolute_error: 0.0758 - val_loss: 0.0167 - val_mean_squared_error: 0.0167 - val_mean_absolute_error: 0.0898\n"
     ]
    }
   ],
   "source": [
    ">>> history = model.fit(X_train, y_train, epochs=150, batch_size=50,  verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mean_squared_error', 'mean_absolute_error', 'val_loss', 'val_mean_squared_error', 'val_mean_absolute_error'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = np.array([[40, 0, 26, 9000, 8000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = 4221.0, Predicted = [14767.]\n"
     ]
    }
   ],
   "source": [
    "Xnew = np.array([[40, 0, 26, 9000, 8000]])\n",
    "Xnew= scaler_x.transform(Xnew)\n",
    "ynew= model.predict(Xnew)\n",
    "#invert normalize\n",
    "ynew = scaler_y.inverse_transform(y_test) \n",
    "Xnew = scaler_x.inverse_transform(X_test)\n",
    "pos = 20\n",
    "print(\"X = %s, Predicted = %s\" % (Xnew[pos, 4], ynew[pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
